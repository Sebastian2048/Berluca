# main.py
import os
import sys
import requests
import logging
import re # Necesario para procesar URLs de GitHub
# üåü CORRECCI√ìN CR√çTICA: A√±adir 'Tuple' a la importaci√≥n de typing
from typing import List, Dict, Set, Any, Tuple 

# üì¶ Importaciones de m√≥dulos locales
try:
    from config import CARPETA_SALIDA, MAX_SERVIDORES_BUSCAR
    from auxiliar import (
        descargar_lista, limpiar_archivos_temporales
    )
    from clasificador import clasificar_enlaces 
    from servidor import auditar_y_balancear_servidores, compilar_inventario_existente
    from auditor_conectividad import auditar_conectividad 

except ImportError as e:
    print(f"ERROR: No se pudo importar un m√≥dulo necesario. Aseg√∫rate de tener todos los archivos (.py) en la misma carpeta.")
    print(f"Detalle del error: {e}")
    sys.exit(1)

logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')

# =========================================================================================
# ‚öôÔ∏è GESTI√ìN DE ENTRADA (M√∫ltiples URLs - GitHub Scraper)
# =========================================================================================

def _url_to_api(repo_url: str) -> Tuple[str, str, str, str]:
    """Convierte una URL de repositorio de GitHub a su URL de API de Contenido."""
    
    # 1. Intentar hacer match con el patr√≥n de directorio/subdirectorio (e.g., /tree/main/path)
    match_branch = re.search(r'github\.com/([^/]+)/([^/]+)/tree/([^/]+)/(.*)', repo_url)
    if match_branch:
        owner, repo, branch, path = match_branch.groups()
        return owner, repo, branch, path
        
    # 2. Intentar hacer match con la ra√≠z del repositorio
    match_root = re.search(r'github\.com/([^/]+)/([^/]+)', repo_url)
    if match_root:
        owner, repo = match_root.groups()
        return owner, repo, 'main', '' # Asumir rama 'main' y path ra√≠z
    
    raise ValueError("URL de GitHub inv√°lida o no soporta el formato de contenido.")


def _get_api_content_url(owner: str, repo: str, branch: str, path: str) -> str:
    """Construye la URL de la API de Contenido."""
    return f"https://api.github.com/repos/{owner}/{repo}/contents/{path}?ref={branch}"


def extraer_urls_m3u_de_github(owner: str, repo: str, branch: str, path: str, urls_encontradas: Set[str]) -> None:
    """Funci√≥n recursiva para extraer URLs M3U de la API de Contenido de GitHub."""
    
    api_url = _get_api_content_url(owner, repo, branch, path)
    
    # Se recomienda usar un User-Agent en peticiones a la API de GitHub
    headers = {'User-Agent': 'Beluga M3U Scraper (v1.0)'}
    
    try:
        response = requests.get(api_url, headers=headers, timeout=20)
        response.raise_for_status() # Lanza HTTPError para 4xx/5xx

        contenido = response.json()
        
        if isinstance(contenido, list):
            for item in contenido:
                if item['type'] == 'file' and item['name'].lower().endswith('.m3u'):
                    # La API proporciona 'download_url', que es la URL RAW.
                    if item.get('download_url'):
                        urls_encontradas.add(item['download_url'])
                
                elif item['type'] == 'dir':
                    # Llamada recursiva al subdirectorio
                    # path_recursivo es el camino relativo que se env√≠a al API
                    path_recursivo = item['path'] 
                    extraer_urls_m3u_de_github(owner, repo, branch, path_recursivo, urls_encontradas)
                    
    except requests.exceptions.RequestException as e:
        logging.error(f"‚ùå Error al acceder a la API de GitHub ({api_url.split('?')[0]}...): {e}")
    except Exception as e:
         logging.error(f"‚ùå Error inesperado al procesar contenido de GitHub: {e}")


def recolectar_urls_desde_repositorio(repo_url: str) -> List[str]:
    """Funci√≥n principal que inicia la extracci√≥n de URLs de GitHub."""
    print(f"\n--- üåê Analizando Repositorio GitHub: {repo_url} ---")
    try:
        owner, repo, branch, path = _url_to_api(repo_url)
        urls_encontradas = set()
        
        extraer_urls_m3u_de_github(owner, repo, branch, path, urls_encontradas)
        
        urls_final = list(urls_encontradas)
        print(f"‚úÖ Se encontraron {len(urls_final)} enlaces M3U en el repositorio.")
        return urls_final
        
    except ValueError as e:
        print(f"‚ùå Error: {e}")
        return []
    except Exception as e:
        print(f"‚ùå Error cr√≠tico al iniciar la recolecci√≥n de GitHub: {e}")
        return []

# =========================================================================================
# ‚öôÔ∏è FLUJO DE CONTROL PRINCIPAL
# =========================================================================================

def ejecutar_proceso_completo(urls: List[str]):
    """Ejecuta el flujo completo para una lista de URLs."""
    rutas_temp = [] 
    print("\n--- üöÄ Iniciando Flujo de Beluga (FASE 1, 2 y 3) ---")

    # 0. COMPILAR INVENTARIO EXISTENTE (FASE 0)
    inventario_existente = compilar_inventario_existente(MAX_SERVIDORES_BUSCAR)
    
    # 1. Obtener todas las listas nuevas (Temporales)
    for i, url in enumerate(urls):
        ruta_temp = os.path.join(CARPETA_SALIDA, f"TEMP_MATERIAL_{i+1:02d}.m3u")
        print(f"\nüîó Recolectando lista {i+1} de {len(urls)} desde: {url}")
        
        # Se asume que auxiliar.descargar_lista existe y es funcional
        if descargar_lista(url, ruta_temp):
            logging.info(f"‚úÖ Lista {i+1} guardada temporalmente en: {ruta_temp}")
            rutas_temp.append(ruta_temp)
        else:
            logging.error(f"‚ùå Fall√≥ la descarga de la lista {i+1}. Omitiendo.")

    if not rutas_temp:
        print("ERROR: No se pudo descargar ninguna lista v√°lida. Saliendo.")
        return

    try:
        # 2. CLASIFICACI√ìN, FUSI√ìN Y CONSOLIDACI√ìN (FASE 1 - Quick Audit)
        print("\n--- üß† Clasificando, Fusionando y Consolidando Inventario (FASE 1 - R√°pida) ---")
        clasificar_enlaces(rutas_temp, inventario_existente) 
        print("‚úÖ Consolidaci√≥n y Quick Audit finalizada. Archivo de resumen listo para Auditor√≠a Lenta.")

        # 3. AUDITOR√çA LENTA (FASE 2 - Streamlink)
        print("\n--- üêå Iniciando Auditor√≠a Lenta (FASE 2 - Streamlink) ---")
        auditar_conectividad()
        print("‚úÖ Auditor√≠a Lenta completada. Archivo de resumen finalizado.")
        
        # 4. BALANCEO ESTRAT√âGICO Y EXCLUSI√ìN (FASE 3)
        print("\n--- ‚öñÔ∏è Iniciando Balanceo Estrat√©gico (FASE 3) ---")
        auditar_y_balancear_servidores(MAX_SERVIDORES_BUSCAR)
            
    except Exception as e:
        print(f"\nERROR CR√çTICO durante el proceso: {e}")
    finally:
        for ruta in rutas_temp:
            limpiar_archivos_temporales(ruta) 
        print("\n--- ‚úÖ Proceso Completo Finalizado ---")
        
# =========================================================================================
# üöÄ PUNTO DE ENTRADA
# =========================================================================================

if __name__ == "__main__":
    
    print("--- üöÄ Iniciando Flujo de Beluga (FASE 1, 2 y 3) ---")
    
    # 1. Solicitar URL del Repositorio/Directorio
    repo_url = input("üîó Ingresa la URL COMPLETA del repositorio/directorio de GitHub para analizar: ").strip()

    if not repo_url:
        print("ERROR: URL no ingresada. Saliendo.")
        sys.exit(0)
    
    # 2. Extraer las URLs M3U del repositorio
    urls_fuente = recolectar_urls_desde_repositorio(repo_url)

    if not urls_fuente:
        print("ERROR: No se encontraron URLs M3U v√°lidas en el repositorio. Saliendo.")
        sys.exit(0)
    
    # 3. Ejecutar el proceso completo
    ejecutar_proceso_completo(urls_fuente)